"""
é£ä¹¦å·¥å…·æ¨¡å—ï¼šå‘é€ç¾¤èŠæ¶ˆæ¯å’Œæ›´æ–°æ–‡æ¡£
"""
import os
import requests
import datetime
from typing import Optional, List
from loguru import logger
from paper import ArxivPaper


def get_tenant_access_token(app_id: str, app_secret: str) -> str:
    """è·å–é£ä¹¦ tenant_access_token"""
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
    payload = {
        "app_id": app_id,
        "app_secret": app_secret
    }
    try:
        resp = requests.post(url, json=payload, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        if data.get("code") != 0:
            raise Exception(f"è·å– token å¤±è´¥: {data.get('msg')}")
        return data["tenant_access_token"]
    except Exception as e:
        logger.error(f"è·å–é£ä¹¦ token å¤±è´¥: {e}")
        raise


def build_paper_summary(p: ArxivPaper) -> dict:
    """æ„å»ºå•ç¯‡è®ºæ–‡çš„æ‘˜è¦ä¿¡æ¯ï¼ˆç”¨äºæ¶ˆæ¯å±•ç¤ºï¼‰"""
    author_list = [a.name for a in p.authors]
    num_authors = len(author_list)
    
    if num_authors <= 3:
        authors = ', '.join(author_list)
    else:
        authors = ', '.join(author_list[:2] + ['...'] + author_list[-1:])
    
    # å¤„ç†å…³é”®è¯
    try:
        kws = getattr(p, "keywords", None)
        if isinstance(kws, list) and len(kws) > 0:
            keywords_str = ', '.join(kws[:4])  # æœ€å¤š4ä¸ª
        else:
            keywords_str = "N/A"
    except Exception:
        keywords_str = "N/A"
    
    # å¤„ç†è¯„åˆ†ï¼ˆæ˜Ÿæ˜Ÿï¼‰
    score = p.score if p.score else 0
    stars = "â­" * min(5, int(score / 2)) if score > 6 else ""
    
    return {
        "title": p.title,
        "authors": authors,
        "keywords": keywords_str,
        "score": score,
        "stars": stars,
        "arxiv_id": p.arxiv_id,
        "tldr": p.tldr,
        "pdf_url": p.pdf_url,
        "code_url": p.code_url,
        "affiliations": p.affiliations or []
    }


def build_feishu_interactive_message(papers: List[ArxivPaper], date_str: Optional[str] = None) -> dict:
    """
    æ„å»ºé£ä¹¦æ¶ˆæ¯ï¼ˆç®€åŒ–ä¸º post ç±»å‹ï¼Œé¿å…å¡ç‰‡ schema æŠ¥ 400ï¼‰
    """
    if date_str is None:
        date_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')

    def build_blocks():
        blocks = []
        if len(papers) == 0:
            blocks.append([{"tag": "text", "text": "ä»Šå¤©æ²¡æœ‰æ–°è®ºæ–‡ï¼Œå¥½å¥½ä¼‘æ¯å§ï¼ğŸ˜Š"}])
            return blocks

        blocks.append([{"tag": "text", "text": f"ğŸ“š Daily arXiv - {date_str}\n", "style": {"bold": True}}])
        blocks.append([{"tag": "text", "text": f"å…±æ¨è {len(papers)} ç¯‡è®ºæ–‡\n\n"}])

        for idx, p in enumerate(papers, 1):
            info = build_paper_summary(p)
            blocks.append([{"tag": "text", "text": f"{idx}. {info['title']} {info['stars']}\n", "style": {"bold": True}}])
            blocks.append([{"tag": "text", "text": f"ä½œè€…: {info['authors']}\n"}])
            blocks.append([{"tag": "text", "text": f"å…³é”®è¯: {info['keywords']}\n"}])
            blocks.append([{"tag": "text", "text": f"TLDR: {info['tldr']}\n"}])

            links = f"arXiv: https://arxiv.org/abs/{info['arxiv_id']}  |  PDF: {info['pdf_url']}"
            if info["code_url"]:
                links += f"  |  Code: {info['code_url']}"
            blocks.append([{"tag": "text", "text": links + "\n"}])
            blocks.append([{"tag": "text", "text": "â€”" * 20 + "\n"}])
        return blocks

    return {
        "msg_type": "post",
        "content": {
            "post": {
                "zh_cn": {
                    "title": f"Daily arXiv - {date_str}",
                    "content": build_blocks()
                }
            }
        }
    }


def build_feishu_post_message(papers: List[ArxivPaper], date_str: Optional[str] = None) -> dict:
    """
    æ„å»ºé£ä¹¦ post ç±»å‹çš„å¯Œæ–‡æœ¬æ¶ˆæ¯ï¼ˆæ”¯æŒæŠ˜å æ•ˆæœï¼‰
    æ¯æ¡è®ºæ–‡å…ˆæ˜¾ç¤ºæ‘˜è¦ï¼Œè¯¦ç»†ä¿¡æ¯åœ¨æŠ˜å åŒºåŸŸ
    """
    if date_str is None:
        date_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    if len(papers) == 0:
        return {
            "msg_type": "post",
            "content": {
                "post": {
                    "zh_cn": {
                        "title": f"ğŸ“š Daily arXiv - {date_str}",
                        "content": [
                            [
                                {
                                    "tag": "text",
                                    "text": "ä»Šå¤©æ²¡æœ‰æ–°è®ºæ–‡ï¼Œå¥½å¥½ä¼‘æ¯å§ï¼ğŸ˜Š"
                                }
                            ]
                        ]
                    }
                }
            }
        }
    
    # æ„å»ºæ¶ˆæ¯å†…å®¹
    content = []
    
    # æ ‡é¢˜è¡Œ
    content.append([
        {
            "tag": "text",
            "text": f"ğŸ“š Daily arXiv - {date_str}\n",
            "style": [
                {"bold": True},
                {"font_size": "large"}
            ]
        }
    ])
    
    content.append([
        {
            "tag": "text",
            "text": f"å…±æ¨è {len(papers)} ç¯‡è®ºæ–‡\n\n",
            "style": [{"font_size": "medium"}]
        }
    ])
    
    # æ¯ç¯‡è®ºæ–‡
    for idx, p in enumerate(papers, 1):
        paper_info = build_paper_summary(p)
        
        # è®ºæ–‡æ ‡é¢˜ï¼ˆå¯ç‚¹å‡»å±•å¼€ï¼‰
        content.append([
            {
                "tag": "text",
                "text": f"{idx}. ",
                "style": [{"bold": True}]
            },
            {
                "tag": "a",
                "text": paper_info["title"],
                "href": f"https://arxiv.org/abs/{paper_info['arxiv_id']}"
            },
            {
                "tag": "text",
                "text": f" {paper_info['stars']}\n",
            }
        ])
        
        # æ‘˜è¦ä¿¡æ¯ï¼ˆä½œè€…ã€å…³é”®è¯ï¼‰
        content.append([
            {
                "tag": "text",
                "text": f"   ä½œè€…: {paper_info['authors']}\n",
                "style": [{"font_size": "small"}]
            }
        ])
        
        content.append([
            {
                "tag": "text",
                "text": f"   å…³é”®è¯: {paper_info['keywords']}\n",
                "style": [{"font_size": "small"}]
            }
        ])
        
        # è¯¦ç»†ä¿¡æ¯ï¼ˆTLDRï¼‰- ä½¿ç”¨åˆ†éš”çº¿
        content.append([
            {
                "tag": "text",
                "text": f"   TLDR: {paper_info['tldr']}\n",
                "style": [{"font_size": "small"}]
            }
        ])
        
        # é“¾æ¥
        links_text = f"   ğŸ“„ PDF: {paper_info['pdf_url']}"
        if paper_info['code_url']:
            links_text += f" | ğŸ’» Code: {paper_info['code_url']}"
        links_text += "\n"
        
        content.append([
            {
                "tag": "text",
                "text": links_text,
                "style": [{"font_size": "small"}]
            }
        ])
        
        # åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ç¯‡ï¼‰
        if idx < len(papers):
            content.append([
                {
                    "tag": "text",
                    "text": "â”€" * 30 + "\n",
                    "style": [{"font_size": "small"}]
                }
            ])
    
    return {
        "msg_type": "post",
        "content": {
            "post": {
                "zh_cn": {
                    "title": f"ğŸ“š Daily arXiv - {date_str}",
                    "content": content
                }
            }
        }
    }


def send_feishu_group_message(
    papers: List[ArxivPaper],
    app_id: str,
    app_secret: str,
    chat_id: str,
    date_str: Optional[str] = None
) -> bool:
    """
    å‘é€é£ä¹¦ç¾¤èŠæ¶ˆæ¯
    
    Args:
        papers: è®ºæ–‡åˆ—è¡¨
        app_id: é£ä¹¦åº”ç”¨ ID
        app_secret: é£ä¹¦åº”ç”¨ Secret
        chat_id: ç¾¤èŠ ID
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        bool: æ˜¯å¦å‘é€æˆåŠŸ
    """
    try:
        token = get_tenant_access_token(app_id, app_secret)
        url = "https://open.feishu.cn/open-apis/im/v1/messages"
        
        # æ„å»ºæ¶ˆæ¯ï¼ˆä½¿ç”¨ interactive ç±»å‹æ”¯æŒæŠ˜å ï¼‰
        message = build_feishu_interactive_message(papers, date_str)
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        params = {
            "receive_id_type": "chat_id"
        }
        
        payload = {
            "receive_id": chat_id,
            **message
        }
        
        resp = requests.post(url, headers=headers, params=params, json=payload, timeout=30)
        try:
            resp.raise_for_status()
        except Exception as http_e:
            logger.error(f"å‘é€é£ä¹¦æ¶ˆæ¯ HTTPé”™è¯¯: {http_e}, å“åº”: {resp.text}")
            return False

        data = resp.json()
        
        if data.get("code") != 0:
            logger.error(f"å‘é€é£ä¹¦æ¶ˆæ¯å¤±è´¥: {data.get('code')} {data.get('msg')} | å“åº”: {data}")
            return False
        
        logger.success(f"âœ… é£ä¹¦ç¾¤èŠæ¶ˆæ¯å‘é€æˆåŠŸ (å…± {len(papers)} ç¯‡è®ºæ–‡)")
        return True
        
    except Exception as e:
        logger.error(f"å‘é€é£ä¹¦æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
        return False


def build_markdown_for_doc(papers: List[ArxivPaper], date_str: Optional[str] = None) -> str:
    """
    æ„å»ºç”¨äºé£ä¹¦æ–‡æ¡£çš„ Markdown å†…å®¹
    """
    if date_str is None:
        date_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    if len(papers) == 0:
        return f"## {date_str}\n\nä»Šå¤©æ²¡æœ‰æ–°è®ºæ–‡ï¼Œå¥½å¥½ä¼‘æ¯å§ï¼ğŸ˜Š\n\n---\n\n"
    
    md_lines = [f"## {date_str}\n"]
    md_lines.append(f"**å…±æ¨è {len(papers)} ç¯‡è®ºæ–‡**\n\n")
    
    for idx, p in enumerate(papers, 1):
        paper_info = build_paper_summary(p)
        
        md_lines.append(f"### {idx}. {paper_info['title']} {paper_info['stars']}\n")
        md_lines.append(f"**ä½œè€…:** {paper_info['authors']}\n\n")
        
        if paper_info['affiliations']:
            affil_str = ', '.join(paper_info['affiliations'][:3])
            if len(paper_info['affiliations']) > 3:
                affil_str += ', ...'
            md_lines.append(f"**æœºæ„:** {affil_str}\n\n")
        
        md_lines.append(f"**å…³é”®è¯:** {paper_info['keywords']}\n\n")
        md_lines.append(f"**TLDR:** {paper_info['tldr']}\n\n")
        md_lines.append(f"**é“¾æ¥:** [arXiv](https://arxiv.org/abs/{paper_info['arxiv_id']}) | [PDF]({paper_info['pdf_url']})")
        
        if paper_info['code_url']:
            md_lines.append(f" | [Code]({paper_info['code_url']})")
        
        md_lines.append("\n\n---\n\n")
    
    return ''.join(md_lines)


def update_feishu_document(
    papers: List[ArxivPaper],
    app_id: str,
    app_secret: str,
    doc_token: str,
    history_file: Optional[str] = None
) -> bool:
    """
    æ›´æ–°é£ä¹¦æ–‡æ¡£ï¼ˆé€šè¿‡ç»´æŠ¤æœ¬åœ° Markdown æ–‡ä»¶ï¼Œç„¶ååŒæ­¥åˆ°é£ä¹¦ï¼‰
    
    Args:
        papers: è®ºæ–‡åˆ—è¡¨
        app_id: é£ä¹¦åº”ç”¨ ID
        app_secret: é£ä¹¦åº”ç”¨ Secret
        doc_token: é£ä¹¦æ–‡æ¡£ token
        history_file: æœ¬åœ°å†å²æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ history.mdï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä¸ç»´æŠ¤æœ¬åœ°æ–‡ä»¶
    
    Returns:
        bool: æ˜¯å¦æ›´æ–°æˆåŠŸ
    """
    try:
        date_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
        new_content = build_markdown_for_doc(papers, date_str)
        
        # 1. æ›´æ–°æœ¬åœ°å†å²æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if history_file:
            try:
                if os.path.exists(history_file):
                    with open(history_file, 'r', encoding='utf-8') as f:
                        existing_content = f.read()
                    # åœ¨æ–‡ä»¶å¼€å¤´æ’å…¥æ–°å†…å®¹
                    with open(history_file, 'w', encoding='utf-8') as f:
                        f.write(new_content + existing_content)
                else:
                    # é¦–æ¬¡åˆ›å»ºï¼Œæ·»åŠ æ ‡é¢˜
                    with open(history_file, 'w', encoding='utf-8') as f:
                        f.write(f"# Daily arXiv æ¨èå†å²\n\n{new_content}")
                logger.info(f"âœ… æœ¬åœ°å†å²æ–‡ä»¶å·²æ›´æ–°: {history_file}")
            except Exception as e:
                logger.warning(f"æ›´æ–°æœ¬åœ°å†å²æ–‡ä»¶å¤±è´¥: {e}")
        
        # 2. å°è¯•ä½¿ç”¨ lark_oapi SDK æ›´æ–°é£ä¹¦æ–‡æ¡£ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
        # æ³¨æ„ï¼šé£ä¹¦æ–‡æ¡£ API æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªåŸºç¡€å®ç°
        # å¦‚æœ SDK ä¸å¯ç”¨ï¼Œä¼šå›é€€åˆ°ä»…ç»´æŠ¤æœ¬åœ°æ–‡ä»¶
        try:
            import lark_oapi as lark
            # åªå¯¼å…¥éœ€è¦çš„ç±»ï¼Œé¿å…å‡½æ•°ä½œç”¨åŸŸå†…ä½¿ç”¨ import *
            from lark_oapi.api.docx.v1 import (
                ListDocumentBlockRequest,
                CreateDocumentBlockChildrenRequest,
            )
            
            client = lark.Client.builder() \
                .app_id(app_id) \
                .app_secret(app_secret) \
                .log_level(lark.LogLevel.INFO) \
                .build()
            
            # è·å–æ–‡æ¡£çš„ç¬¬ä¸€ä¸ª blockï¼ˆç”¨äºåœ¨å¼€å¤´æ’å…¥æ–°å†…å®¹ï¼‰
            blocks_request = ListDocumentBlockRequest.builder() \
                .document_id(doc_token) \
                .page_size(10) \
                .build()
            
            blocks_response = client.docx.v1.document_block.list(blocks_request)
            if not blocks_response.success():
                raise Exception(f"è·å–æ–‡æ¡£ blocks å¤±è´¥: {blocks_response.msg}")
            
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª block çš„ IDï¼ˆç”¨äºæ’å…¥ä½ç½®ï¼‰
            first_block_id = None
            if blocks_response.data and blocks_response.data.items:
                first_block_id = blocks_response.data.items[0].block_id
            
            # å°† Markdown å†…å®¹è½¬æ¢ä¸ºé£ä¹¦æ–‡æ¡£ blocks
            # ç®€åŒ–å¤„ç†ï¼šå°†æ¯æ®µå†…å®¹è½¬æ¢ä¸ºæ–‡æœ¬ block
            import re
            paragraphs = [p.strip() for p in new_content.split('\n\n') if p.strip()]
            blocks_to_insert = []
            
            for para in paragraphs:
                if para.startswith('##'):
                    # äºŒçº§æ ‡é¢˜
                    text = re.sub(r'^##+\s*', '', para).strip()
                    blocks_to_insert.append({
                        "block_type": 2,  # æ–‡æœ¬å—
                        "text": {
                            "elements": [{
                                "text_run": {
                                    "content": text,
                                    "style": {"bold": True}
                                }
                            }]
                        }
                    })
                elif para.startswith('---'):
                    # åˆ†éš”çº¿
                    blocks_to_insert.append({"block_type": 19})  # åˆ†éš”çº¿å—
                else:
                    # æ™®é€šæ–‡æœ¬æ®µè½ï¼Œå¤„ç†é“¾æ¥
                    text_runs = []
                    last_end = 0
                    link_pattern = r'\[([^\]]+)\]\(([^\)]+)\)'
                    
                    for match in re.finditer(link_pattern, para):
                        if match.start() > last_end:
                            text_runs.append({
                                "text_run": {"content": para[last_end:match.start()]}
                            })
                        link_text = match.group(1)
                        link_url = match.group(2)
                        text_runs.append({
                            "text_run": {
                                "content": link_text,
                                "style": {"link": {"url": link_url}}
                            }
                        })
                        last_end = match.end()
                    
                    if last_end < len(para):
                        text_runs.append({
                            "text_run": {"content": para[last_end:]}
                        })
                    
                    if not text_runs:
                        text_runs = [{"text_run": {"content": para}}]
                    
                    blocks_to_insert.append({
                        "block_type": 2,
                        "text": {"elements": text_runs}
                    })
            
            # åœ¨æ–‡æ¡£å¼€å¤´æ’å…¥æ–°å†…å®¹
            if first_block_id and blocks_to_insert:
                insert_request = CreateDocumentBlockChildrenRequest.builder() \
                    .document_id(doc_token) \
                    .block_id(first_block_id) \
                    .index(0) \
                    .children(blocks_to_insert) \
                    .build()
                
                insert_response = client.docx.v1.document_block_children.create(insert_request)
                if insert_response.success():
                    logger.success(f"âœ… é£ä¹¦æ–‡æ¡£æ›´æ–°æˆåŠŸ")
                    return True
                else:
                    raise Exception(f"æ›´æ–°å¤±è´¥: {insert_response.msg}")
            else:
                raise Exception("æ— æ³•æ‰¾åˆ°æ’å…¥ä½ç½®æˆ–å†…å®¹ä¸ºç©º")
                
        except ImportError:
            logger.warning("âš ï¸  lark_oapi æœªå®‰è£…ï¼Œæ— æ³•è‡ªåŠ¨æ›´æ–°é£ä¹¦æ–‡æ¡£")
            logger.info("   å»ºè®®ï¼šå®‰è£… lark_oapi: pip install lark-oapi")
            if history_file:
                logger.info(f"   å†…å®¹å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: {history_file}")
                logger.info("   ä½ å¯ä»¥æ‰‹åŠ¨å°† Markdown å†…å®¹å¯¼å…¥åˆ°é£ä¹¦æ–‡æ¡£ï¼ˆé£ä¹¦æ”¯æŒ Markdown å¯¼å…¥ï¼‰")
            return True  # æœ¬åœ°æ–‡ä»¶å·²æ›´æ–°ï¼Œè¿”å›æˆåŠŸ
        except Exception as e:
            logger.warning(f"âš ï¸  é£ä¹¦æ–‡æ¡£è‡ªåŠ¨æ›´æ–°å¤±è´¥: {e}")
            if history_file:
                logger.info(f"   å†…å®¹å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: {history_file}")
                logger.info("   å»ºè®®ï¼šæ‰‹åŠ¨å°† Markdown å†…å®¹å¯¼å…¥åˆ°é£ä¹¦æ–‡æ¡£ï¼ˆé£ä¹¦æ”¯æŒ Markdown å¯¼å…¥ï¼‰")
            return True  # æœ¬åœ°æ–‡ä»¶å·²æ›´æ–°ï¼Œè¿”å›æˆåŠŸ
        
    except Exception as e:
        logger.error(f"æ›´æ–°é£ä¹¦æ–‡æ¡£æ—¶å‡ºé”™: {e}")
        return False

