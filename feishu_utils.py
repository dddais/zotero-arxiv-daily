"""
é£ä¹¦å·¥å…·æ¨¡å—ï¼šå‘é€ç¾¤èŠæ¶ˆæ¯å’Œæ›´æ–°æ–‡æ¡£
"""
import os
import requests
import datetime
from typing import Optional, List
from loguru import logger
from paper import ArxivPaper


def get_tenant_access_token(app_id: str, app_secret: str) -> str:
    """è·å–é£ä¹¦ tenant_access_token"""
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
    payload = {
        "app_id": app_id,
        "app_secret": app_secret
    }
    try:
        resp = requests.post(url, json=payload, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        if data.get("code") != 0:
            raise Exception(f"è·å– token å¤±è´¥: {data.get('msg')}")
        return data["tenant_access_token"]
    except Exception as e:
        logger.error(f"è·å–é£ä¹¦ token å¤±è´¥: {e}")
        raise


def build_paper_summary(p: ArxivPaper) -> dict:
    """æ„å»ºå•ç¯‡è®ºæ–‡çš„æ‘˜è¦ä¿¡æ¯ï¼ˆç”¨äºæ¶ˆæ¯å±•ç¤ºï¼‰"""
    author_list = [a.name for a in p.authors]
    num_authors = len(author_list)
    
    if num_authors <= 3:
        authors = ', '.join(author_list)
    else:
        authors = ', '.join(author_list[:2] + ['...'] + author_list[-1:])
    
    # å¤„ç†å…³é”®è¯
    try:
        kws = getattr(p, "keywords", None)
        if isinstance(kws, list) and len(kws) > 0:
            keywords_str = ', '.join(kws[:4])  # æœ€å¤š4ä¸ª
        else:
            keywords_str = "N/A"
    except Exception:
        keywords_str = "N/A"
    
    # å¤„ç†è¯„åˆ†ï¼ˆæ˜Ÿæ˜Ÿï¼‰
    score = p.score if p.score else 0
    stars = "â­" * min(5, int(score / 2)) if score > 6 else ""
    
    return {
        "title": p.title,
        "authors": authors,
        "keywords": keywords_str,
        "score": score,
        "stars": stars,
        "arxiv_id": p.arxiv_id,
        "tldr": p.tldr,
        "pdf_url": p.pdf_url,
        "code_url": p.code_url,
        "affiliations": p.affiliations or []
    }


def build_feishu_interactive_message(
    papers: List[ArxivPaper],
    date_str: Optional[str] = None,
    doc_url: Optional[str] = None,
) -> dict:
    """
    æ„å»ºé£ä¹¦ interactive å¡ç‰‡ï¼šç²¾ç®€æ‘˜è¦ï¼ˆTop 3ï¼‰+ æŸ¥çœ‹è¯¦æƒ…æŒ‰é’®
    é‡‡ç”¨å®˜æ–¹æ¨èçš„ç®€å• card ç»“æ„ï¼šè‹¥ä»æœ‰é—®é¢˜ï¼Œå¯ä» API Explorer è¿›ä¸€æ­¥å¾®è°ƒã€‚
    """
    if date_str is None:
        date_str = datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

    title = f"Daily arXiv - {date_str}"

    if len(papers) == 0:
        summary_md = "ä»Šå¤©æ²¡æœ‰æ–°è®ºæ–‡ï¼Œå¥½å¥½ä¼‘æ¯å§ï¼ğŸ˜Š"
    else:
        lines = [
            f"ğŸ“š **{title}**",
            "",
            f"ä»Šæ—¥æ¨è {len(papers)} ç¯‡è®ºæ–‡ï¼Œä¸‹é¢æ˜¯å‰ 3 ç¯‡ç®€è¦ä¿¡æ¯ï¼š",
            "",
        ]
        for idx, p in enumerate(papers[:3], 1):
            info = build_paper_summary(p)
            one = [
                f"{idx}. **{info['title']}** {info['stars']}",
                f"   å…³é”®è¯: {info['keywords']}",
                f"   [arXiv é“¾æ¥](https://arxiv.org/abs/{info['arxiv_id']})",
                "",
            ]
            lines.extend(one)
        if doc_url:
            lines.append(f"[ğŸ‘‰ æŸ¥çœ‹å…¨éƒ¨è¯¦æƒ…ï¼ˆé£ä¹¦æ–‡æ¡£ï¼‰]({doc_url})")
        summary_md = "\n".join(lines).strip()

    elements: list[dict] = [
        {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": summary_md,
            },
        }
    ]

    # /im/v1/messages å¯¹ interactive çš„è¦æ±‚æ˜¯ï¼š
    # msg_type="interactive"ï¼Œcontent ä¸º JSON å­—ç¬¦ä¸²å½¢å¼çš„ card å¯¹è±¡
    import json
    card_obj = {
        "config": {"wide_screen_mode": True},
        "header": {
            "title": {"tag": "plain_text", "content": title},
            "template": "blue",
        },
        "elements": elements,
    }

    return {
        "msg_type": "interactive",
        "content": json.dumps(card_obj, ensure_ascii=False),
    }


def build_feishu_post_message(papers: List[ArxivPaper], date_str: Optional[str] = None) -> dict:
    """
    æ„å»ºé£ä¹¦ post ç±»å‹çš„å¯Œæ–‡æœ¬æ¶ˆæ¯ï¼ˆæ”¯æŒæŠ˜å æ•ˆæœï¼‰
    æ¯æ¡è®ºæ–‡å…ˆæ˜¾ç¤ºæ‘˜è¦ï¼Œè¯¦ç»†ä¿¡æ¯åœ¨æŠ˜å åŒºåŸŸ
    """
    if date_str is None:
        date_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    if len(papers) == 0:
        return {
            "msg_type": "post",
            "content": {
                "post": {
                    "zh_cn": {
                        "title": f"ğŸ“š Daily arXiv - {date_str}",
                        "content": [
                            [
                                {
                                    "tag": "text",
                                    "text": "ä»Šå¤©æ²¡æœ‰æ–°è®ºæ–‡ï¼Œå¥½å¥½ä¼‘æ¯å§ï¼ğŸ˜Š"
                                }
                            ]
                        ]
                    }
                }
            }
        }
    
    # æ„å»ºæ¶ˆæ¯å†…å®¹
    content = []
    
    # æ ‡é¢˜è¡Œ
    content.append([
        {
            "tag": "text",
            "text": f"ğŸ“š Daily arXiv - {date_str}\n",
            "style": [
                {"bold": True},
                {"font_size": "large"}
            ]
        }
    ])
    
    content.append([
        {
            "tag": "text",
            "text": f"å…±æ¨è {len(papers)} ç¯‡è®ºæ–‡\n\n",
            "style": [{"font_size": "medium"}]
        }
    ])
    
    # æ¯ç¯‡è®ºæ–‡
    for idx, p in enumerate(papers, 1):
        paper_info = build_paper_summary(p)
        
        # è®ºæ–‡æ ‡é¢˜ï¼ˆå¯ç‚¹å‡»å±•å¼€ï¼‰
        content.append([
            {
                "tag": "text",
                "text": f"{idx}. ",
                "style": [{"bold": True}]
            },
            {
                "tag": "a",
                "text": paper_info["title"],
                "href": f"https://arxiv.org/abs/{paper_info['arxiv_id']}"
            },
            {
                "tag": "text",
                "text": f" {paper_info['stars']}\n",
            }
        ])
        
        # æ‘˜è¦ä¿¡æ¯ï¼ˆä½œè€…ã€å…³é”®è¯ï¼‰
        content.append([
            {
                "tag": "text",
                "text": f"   ä½œè€…: {paper_info['authors']}\n",
                "style": [{"font_size": "small"}]
            }
        ])
        
        content.append([
            {
                "tag": "text",
                "text": f"   å…³é”®è¯: {paper_info['keywords']}\n",
                "style": [{"font_size": "small"}]
            }
        ])
        
        # è¯¦ç»†ä¿¡æ¯ï¼ˆTLDRï¼‰- ä½¿ç”¨åˆ†éš”çº¿
        content.append([
            {
                "tag": "text",
                "text": f"   TLDR: {paper_info['tldr']}\n",
                "style": [{"font_size": "small"}]
            }
        ])
        
        # é“¾æ¥
        links_text = f"   ğŸ“„ PDF: {paper_info['pdf_url']}"
        if paper_info['code_url']:
            links_text += f" | ğŸ’» Code: {paper_info['code_url']}"
        links_text += "\n"
        
        content.append([
            {
                "tag": "text",
                "text": links_text,
                "style": [{"font_size": "small"}]
            }
        ])
        
        # åˆ†éš”çº¿ï¼ˆé™¤äº†æœ€åä¸€ç¯‡ï¼‰
        if idx < len(papers):
            content.append([
                {
                    "tag": "text",
                    "text": "â”€" * 30 + "\n",
                    "style": [{"font_size": "small"}]
                }
            ])
    
    return {
        "msg_type": "post",
        "content": {
            "post": {
                "zh_cn": {
                    "title": f"ğŸ“š Daily arXiv - {date_str}",
                    "content": content
                }
            }
        }
    }


def send_feishu_group_message(
    papers: List[ArxivPaper],
    app_id: str,
    app_secret: str,
    chat_id: str,
    date_str: Optional[str] = None,
    doc_url: Optional[str] = None,
) -> bool:
    """
    å‘é€é£ä¹¦ç¾¤èŠæ¶ˆæ¯
    
    Args:
        papers: è®ºæ–‡åˆ—è¡¨
        app_id: é£ä¹¦åº”ç”¨ ID
        app_secret: é£ä¹¦åº”ç”¨ Secret
        chat_id: ç¾¤èŠ ID
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        bool: æ˜¯å¦å‘é€æˆåŠŸ
    """
    try:
        token = get_tenant_access_token(app_id, app_secret)
        url = "https://open.feishu.cn/open-apis/im/v1/messages"
        
        # æ„å»ºæ¶ˆæ¯ï¼ˆå¡ç‰‡æ¦‚è¦ + æŸ¥çœ‹è¯¦æƒ…æŒ‰é’®ï¼‰
        message = build_feishu_interactive_message(papers, date_str, doc_url)
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        params = {
            "receive_id_type": "chat_id"
        }
        
        payload = {
            "receive_id": chat_id,
            **message
        }

        resp = requests.post(url, headers=headers, params=params, json=payload, timeout=30)
        try:
            resp.raise_for_status()
        except Exception as http_e:
            logger.error(f"å‘é€é£ä¹¦æ¶ˆæ¯ HTTPé”™è¯¯: {http_e}, å“åº”: {resp.text}")
            return False

        data = resp.json()
        
        if data.get("code") != 0:
            logger.error(f"å‘é€é£ä¹¦æ¶ˆæ¯å¤±è´¥: {data.get('code')} {data.get('msg')} | å“åº”: {data}")
            return False
        
        logger.success(f"âœ… é£ä¹¦ç¾¤èŠæ¶ˆæ¯å‘é€æˆåŠŸ (å…± {len(papers)} ç¯‡è®ºæ–‡)")
        return True
        
    except Exception as e:
        logger.error(f"å‘é€é£ä¹¦æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
        return False


def build_markdown_for_doc(papers: List[ArxivPaper], date_str: Optional[str] = None) -> str:
    """
    æ„å»ºç”¨äºé£ä¹¦æ–‡æ¡£çš„ Markdown å†…å®¹
    """
    if date_str is None:
        date_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    if len(papers) == 0:
        return f"## {date_str}\n\nä»Šå¤©æ²¡æœ‰æ–°è®ºæ–‡ï¼Œå¥½å¥½ä¼‘æ¯å§ï¼ğŸ˜Š\n\n---\n\n"
    
    md_lines = [f"## {date_str}\n"]
    md_lines.append(f"**å…±æ¨è {len(papers)} ç¯‡è®ºæ–‡**\n\n")
    
    for idx, p in enumerate(papers, 1):
        paper_info = build_paper_summary(p)
        
        md_lines.append(f"### {idx}. {paper_info['title']} {paper_info['stars']}\n")
        md_lines.append(f"**ä½œè€…:** {paper_info['authors']}\n\n")
        
        if paper_info['affiliations']:
            affil_str = ', '.join(paper_info['affiliations'][:3])
            if len(paper_info['affiliations']) > 3:
                affil_str += ', ...'
            md_lines.append(f"**æœºæ„:** {affil_str}\n\n")
        
        md_lines.append(f"**å…³é”®è¯:** {paper_info['keywords']}\n\n")
        md_lines.append(f"**TLDR:** {paper_info['tldr']}\n\n")
        md_lines.append(f"**é“¾æ¥:** [arXiv](https://arxiv.org/abs/{paper_info['arxiv_id']}) | [PDF]({paper_info['pdf_url']})")
        
        if paper_info['code_url']:
            md_lines.append(f" | [Code]({paper_info['code_url']})")
        
        md_lines.append("\n\n---\n\n")
    
    return ''.join(md_lines)


def update_feishu_document(
    papers: List[ArxivPaper],
    app_id: str,
    app_secret: str,
    doc_token: str,
    history_file: Optional[str] = None
) -> bool:
    """
    æ›´æ–°é£ä¹¦æ–‡æ¡£ï¼ˆé€šè¿‡ç»´æŠ¤æœ¬åœ° Markdown æ–‡ä»¶ï¼Œç„¶ååŒæ­¥åˆ°é£ä¹¦ï¼‰
    
    Args:
        papers: è®ºæ–‡åˆ—è¡¨
        app_id: é£ä¹¦åº”ç”¨ ID
        app_secret: é£ä¹¦åº”ç”¨ Secret
        doc_token: é£ä¹¦æ–‡æ¡£ token
        history_file: æœ¬åœ°å†å²æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ history.mdï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä¸ç»´æŠ¤æœ¬åœ°æ–‡ä»¶
    
    Returns:
        bool: æ˜¯å¦æ›´æ–°æˆåŠŸ
    """
    try:
        date_str = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
        new_content = build_markdown_for_doc(papers, date_str)
        
        # 1. æ›´æ–°æœ¬åœ°å†å²æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if history_file:
            try:
                if os.path.exists(history_file):
                    with open(history_file, 'r', encoding='utf-8') as f:
                        existing_content = f.read()
                    # åœ¨æ–‡ä»¶å¼€å¤´æ’å…¥æ–°å†…å®¹
                    with open(history_file, 'w', encoding='utf-8') as f:
                        f.write(new_content + existing_content)
                else:
                    # é¦–æ¬¡åˆ›å»ºï¼Œæ·»åŠ æ ‡é¢˜
                    with open(history_file, 'w', encoding='utf-8') as f:
                        f.write(f"# Daily arXiv æ¨èå†å²\n\n{new_content}")
                logger.info(f"âœ… æœ¬åœ°å†å²æ–‡ä»¶å·²æ›´æ–°: {history_file}")
            except Exception as e:
                logger.warning(f"æ›´æ–°æœ¬åœ°å†å²æ–‡ä»¶å¤±è´¥: {e}")
        
        # 2. ä½¿ç”¨ doc/v2 è¦†ç›–æ›´æ–°é£ä¹¦æ–‡æ¡£å†…å®¹ï¼ˆwiki é“¾æ¥å¯¹åº”çš„åº•å±‚æ–‡æ¡£ï¼‰
        try:
            # å‡†å¤‡å®Œæ•´å†…å®¹ï¼šä¼˜å…ˆä½¿ç”¨æœ¬åœ° history æ–‡ä»¶ï¼Œè‹¥ä¸å­˜åœ¨åˆ™ç”¨å½“å‰ new_content
            if history_file and os.path.exists(history_file):
                with open(history_file, "r", encoding="utf-8") as f:
                    full_content = f.read()
            else:
                full_content = f"# Daily arXiv æ¨èå†å²\n\n{new_content}"

            # è·å– tenant_access_token
            token = get_tenant_access_token(app_id, app_secret)

            # doc_token æ¥è‡ªä½ çš„ wiki URL: https://x2-robot.feishu.cn/wiki/{doc_token}
            url = f"https://open.feishu.cn/open-apis/doc/v2/{doc_token}/content"
            # url = f"https://x2-robot.feishu.cn/wiki/{doc_token}"
            headers = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json",
            }
            payload = {"content": full_content}

            resp = requests.put(url, headers=headers, json=payload, timeout=30)
            try:
                resp.raise_for_status()
            except Exception as http_e:
                logger.warning(f"âš ï¸  é£ä¹¦æ–‡æ¡£ HTTP æ›´æ–°å¤±è´¥: {http_e}, å“åº”: {resp.text}")
                return True  # æœ¬åœ°æ–‡ä»¶å·²æ›´æ–°ï¼Œè§†ä¸ºéƒ¨åˆ†æˆåŠŸ

            data = resp.json()
            if data.get("code") != 0:
                logger.warning(f"âš ï¸  é£ä¹¦æ–‡æ¡£ API è¿”å›é”™è¯¯: {data.get('code')} {data.get('msg')} | å“åº”: {data}")
                return True  # æœ¬åœ°æ–‡ä»¶å·²æ›´æ–°ï¼Œè§†ä¸ºéƒ¨åˆ†æˆåŠŸ

            logger.success("âœ… é£ä¹¦æ–‡æ¡£æ›´æ–°æˆåŠŸï¼ˆdoc/v2 è¦†ç›–æ¨¡å¼ï¼‰")
            return True

        except Exception as e:
            logger.warning(f"âš ï¸  é£ä¹¦æ–‡æ¡£è‡ªåŠ¨æ›´æ–°å¤±è´¥: {e}")
            if history_file:
                logger.info(f"   å†…å®¹å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: {history_file}")
                logger.info("   å»ºè®®ï¼šæ‰‹åŠ¨å°† Markdown å†…å®¹å¯¼å…¥åˆ°é£ä¹¦æ–‡æ¡£ï¼ˆé£ä¹¦æ”¯æŒ Markdown å¯¼å…¥ï¼‰")
            return True
        
    except Exception as e:
        logger.error(f"æ›´æ–°é£ä¹¦æ–‡æ¡£æ—¶å‡ºé”™: {e}")
        return False

